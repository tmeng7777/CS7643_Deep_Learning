# Lesson 7: Visualization

## Visualization of Neural Networks

### Visualization Neural Networks

Given a __trained__ model, we'd like to understand what it learned
- Weights
- Activations
- Gradients
- Robustness

![img](imgs/M2L07_01.png)

### Visualizing Weights

__FC Layer:__ (if connected to image itself) Reshape weights for a node back into size of image, scale 0-255
![img](imgs/M2L07_02.png)

__Conv layers:__ For each kernel, scale values from 0-255 and visualize
![img](imgs/M2L07_03.png)

*Problem: 3x3 filters difficult to interpret!*

### Visualizing Output Maps

We can also produce __visualization output (aka activation/filter) maps__

*There are __larger__ early in the network*

![img](imgs/M2L07_04.png)

![img](imgs/M2L07_05.png)

### Activations - Small Output Sizes

![img](imgs/M2L07_06.png)

*Problem: Small conv outputs also hard to interpret*

### CNN101 and CNN Explainer

![img](imgs/M2L07_07.png)

- [CNN-Explainer](https://poloclub.github.io/cnn-explainer/)
- [CNN 101: Interactive Visual Learning for Convolutional Neural Networks](https://fredhohman.com/papers/cnn101)

### Dimensionality Reduction: t-SNE

We can take the activations of any layer (FC, conv, etc.) and __perform dimensionality reduction__
- Often reduce to two dimensions for plotting
- E.g. using Principle Component Analysis (PCA)

__t-SNE is most common__
- Performs non-linear mapping to preserve pair-wise distances

![img](imgs/M2L07_08.png)

### Visualizing Neural Networks

![img](imgs/M2L07_09.png)

### Summary & Caveats
While these methods provide __some__ visually interpretable representations, they can be misleading or uninformative (Adebayo et al., 2018)

Assessing interpretability is difficult
- Requires __user studies__ to show __usefulness__
- E.g. they allow a user to predict mistakes beforehand

Neural networks learn __distributed representations__
- (no one node represents a particular feature)
- This makes interpretation difficult

*Adebayo et al., “Sanity Checks for Saliency Maps”, 2018.*


### References and Links:
- Fei-Fei Li, Justin Johnson, Serena Yeung, from CS 231n
- Zeiler & Fergus, 2014
- Simonyan et al, 2013
- Hendrycks & Dietterich, 2019
- Yosinski et al., “Understanding Neural Networks Through Deep Visualization”, 2015
- https://poloclub.github.io/cnn-explainer/Links to an external site.
- https://fredhohman.com/papers/cnn101 Links to an external site.
- Van der Maaten & Hinton, “Visualizing Data using t-SNE”, 2008.
- Adebayo et al., “Sanity Checks for Saliency Maps”, 2018.


## Gradient-Based Visualizations

### Visualizing Neural Networks

Backwards pass gives us __gradients__ for all layers: How the loss changes as we change different parts of the inpu

This can be __useful not just for optimization__, but also to understand what was learned
- Gradient of __loss__ with respect to __all layers__ (including input!)
- Gradient of __any layer__ with respect to __input__ (by cutting off computation graph)

![img](imgs/M2L07_10.png)

### Gradient of Loss w.r.t. Image

__Idea:__ We can backprop to the image
- Sensitivity of loss to individual pixel changes
- Large sensitivity implies important pixels
- Called __Saliency Maps__

__In practice:__
- Instead of loss, find gradient of classifier __scores__ (pre-softmax)
- Take absolute value of gradient
- Sum across all channels

![img](imgs/M2L07_11.png)

### Object Segmentation for Free!

Applying traditional (non-learned) computer vision segmentation algorithms on gradients gets us __object segmentation for free!__

Surprising because __not part of supervision__

![img](imgs/M2L07_12.png)

### Detecting Bias

Can be used to __detect dataset bias__
- E.g. snow used to misclassify as wolf

![img](imgs/M2L07_13.png)

__Incorrect predictions__ also informative

### Gradient of Activation with respect to Input

Rather than loss or scores, we can pick a neuron somewhere deep in the network and compute gradient of __activation__ with respect to input

__Steps:__
- Pick a neuron
- Find gradient of its activation w.r.t. input image
- Can first find highest activated image patches using its corresponding neuron (based on receptive field)

![img](imgs/M2L07_14.png)

*More info in Q&A: __How is Gradient of any layer with respect of input calculated?__*


### Guided Backprop

Normal backprop not always best choise

__Example:__ You may get parts of image that __decrease__ the feature activation
- There are probably lots of such input pixels

__Guided backprop__ can be used to improve visualizations

![img](imgs/M2L07_15.png)

![img](imgs/M2L07_16.png)

*More info in Q&A: __How does Guided Backpropagation work?__*

__VGG Layer-by-Layer Visualization__
- ![img](imgs/M2L07_17.png)
- ![img](imgs/M2L07_18.png)
- ![img](imgs/M2L07_19.png)
- ![img](imgs/M2L07_20.png)

*More info in Q&A: __What do you learn from VGG layer-by-layer visualization generated by deconvolution?__*

### Grad-CAM

__Grad-CAM__
![img](imgs/M2L07_21.png)

*More info in Q&A: __How dose Grad-CAM work?__*


__Guided Grad-CAM__
![img](imgs/M2L07_22.png)

*More info in Q&A: __How dose Guided Grad-CAM work?__*

![img](imgs/M2L07_23.png)
![img](imgs/M2L07_24.png)

### Summary
- Gradients are important __not just for optimization__, but also for __analyzing__ what neural networks __have learned__
- Standard backprop __not always the most informative__ for visualization purposes
- Several ways to __modify the gradient flow__ to improve visualization results

*More info in Q&A: __What are some most advanced gradient visualization methods, when and where are they published?__*

### References and Links:

- Simonyan et al., “Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps”, 2013
- Ribeiro et al., "Why Should I Trust You?": Explaining the Predictions of Any Classifier
- Springenberg et al., “Striving For Simplicity: The All Convolutional Net"
- Visualizing and Understanding Convolutional Networks, Zeiler & Fergus, 2014.
- Selfvaraju et al., Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization, 2016.



