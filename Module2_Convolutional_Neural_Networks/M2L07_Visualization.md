# Lesson 7: Visualization

## Visualization of Neural Networks

### Visualization Neural Networks

Given a __trained__ model, we'd like to understand what it learned
- Weights
- Activations
- Gradients
- Robustness

![img](imgs/M2L07_01.png)

### Visualizing Weights

__FC Layer:__ (if connected to image itself) Reshape weights for a node back into size of image, scale 0-255
![img](imgs/M2L07_02.png)

__Conv layers:__ For each kernel, scale values from 0-255 and visualize
![img](imgs/M2L07_03.png)

*Problem: 3x3 filters difficult to interpret!*

### Visualizing Output Maps

We can also produce __visualization output (aka activation/filter) maps__

*There are __larger__ early in the network*

![img](imgs/M2L07_04.png)

![img](imgs/M2L07_05.png)

### Activations - Small Output Sizes

![img](imgs/M2L07_06.png)

*Problem: Small conv outputs also hard to interpret*

### CNN101 and CNN Explainer

![img](imgs/M2L07_07.png)

- [CNN-Explainer](https://poloclub.github.io/cnn-explainer/)
- [CNN 101: Interactive Visual Learning for Convolutional Neural Networks](https://fredhohman.com/papers/cnn101)

### Dimensionality Reduction: t-SNE

We can take the activations of any layer (FC, conv, etc.) and __perform dimensionality reduction__
- Often reduce to two dimensions for plotting
- E.g. using Principle Component Analysis (PCA)

__t-SNE is most common__
- Performs non-linear mapping to preserve pair-wise distances

![img](imgs/M2L07_08.png)

### Visualizing Neural Networks

![img](imgs/M2L07_09.png)

### Summary & Caveats
While these methods provide __some__ visually interpretable representations, they can be misleading or uninformative (Adebayo et al., 2018)

Assessing interpretability is difficult
- Requires __user studies__ to show __usefulness__
- E.g. they allow a user to predict mistakes beforehand

Neural networks learn __distributed representations__
- (no one node represents a particular feature)
- This makes interpretation difficult

*Adebayo et al., “Sanity Checks for Saliency Maps”, 2018.*


### References and Links:
- Fei-Fei Li, Justin Johnson, Serena Yeung, from CS 231n
- Zeiler & Fergus, 2014
- Simonyan et al, 2013
- Hendrycks & Dietterich, 2019
- Yosinski et al., “Understanding Neural Networks Through Deep Visualization”, 2015
- https://poloclub.github.io/cnn-explainer/Links to an external site.
- https://fredhohman.com/papers/cnn101 Links to an external site.
- Van der Maaten & Hinton, “Visualizing Data using t-SNE”, 2008.
- Adebayo et al., “Sanity Checks for Saliency Maps”, 2018.


## Gradient-Based Visualizations

### Visualizing Neural Networks

Backwards pass gives us __gradients__ for all layers: How the loss changes as we change different parts of the inpu

This can be __useful not just for optimization__, but also to understand what was learned
- Gradient of __loss__ with respect to __all layers__ (including input!)
- Gradient of __any layer__ with respect to __input__ (by cutting off computation graph)

![img](imgs/M2L07_10.png)

### Gradient of Loss w.r.t. Image

__Idea:__ We can backprop to the image
- Sensitivity of loss to individual pixel changes
- Large sensitivity implies important pixels
- Called __Saliency Maps__

__In practice:__
- Instead of loss, find gradient of classifier __scores__ (pre-softmax)
- Take absolute value of gradient
- Sum across all channels

![img](imgs/M2L07_11.png)

### Object Segmentation for Free!

Applying traditional (non-learned) computer vision segmentation algorithms on gradients gets us __object segmentation for free!__

Surprising because __not part of supervision__

![img](imgs/M2L07_12.png)

### Detecting Bias

Can be used to __detect dataset bias__
- E.g. snow used to misclassify as wolf

![img](imgs/M2L07_13.png)

__Incorrect predictions__ also informative

### Gradient of Activation with respect to Input

Rather than loss or scores, we can pick a neuron somewhere deep in the network and compute gradient of __activation__ with respect to input

__Steps:__
- Pick a neuron
- Find gradient of its activation w.r.t. input image
- Can first find highest activated image patches using its corresponding neuron (based on receptive field)

![img](imgs/M2L07_14.png)

*More info in Q&A: __How is Gradient of any layer with respect of input calculated?__*


### Guided Backprop

Normal backprop not always best choise

__Example:__ You may get parts of image that __decrease__ the feature activation
- There are probably lots of such input pixels

__Guided backprop__ can be used to improve visualizations

![img](imgs/M2L07_15.png)

![img](imgs/M2L07_16.png)

*More info in Q&A: __How does Guided Backpropagation work?__*

__VGG Layer-by-Layer Visualization__
- ![img](imgs/M2L07_17.png)
- ![img](imgs/M2L07_18.png)
- ![img](imgs/M2L07_19.png)
- ![img](imgs/M2L07_20.png)

*More info in Q&A: __What do you learn from VGG layer-by-layer visualization generated by deconvolution?__*

### Grad-CAM

__Grad-CAM__
![img](imgs/M2L07_21.png)

*More info in Q&A: __How dose Grad-CAM work?__*


__Guided Grad-CAM__
![img](imgs/M2L07_22.png)

*More info in Q&A: __How dose Guided Grad-CAM work?__*

![img](imgs/M2L07_23.png)
![img](imgs/M2L07_24.png)

### Summary
- Gradients are important __not just for optimization__, but also for __analyzing__ what neural networks __have learned__
- Standard backprop __not always the most informative__ for visualization purposes
- Several ways to __modify the gradient flow__ to improve visualization results

*More info in Q&A: __What are some most advanced gradient visualization methods, when and where are they published?__*

### References and Links:

- Simonyan et al., “Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps”, 2013
- Ribeiro et al., "Why Should I Trust You?": Explaining the Predictions of Any Classifier
- Springenberg et al., “Striving For Simplicity: The All Convolutional Net"
- Visualizing and Understanding Convolutional Networks, Zeiler & Fergus, 2014.
- Selfvaraju et al., Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization, 2016.


## Optimizing the Input Images

### Optimizing the Image

__Idea:__ Since we have the gradient of scores w.r.t. inputs, can we *optimize* the image itself to maximize the score?

__Why?__
- Generate images from scratch!
- Adversarial examples

![img](imgs/M2L07_25.png)

### Gradient Ascent on the Scores

We can perform __gradient ascent__ on image
- Start from random/zero image
- Use scores to avoid minimizing other class scores instead

Often need __regularization term__ to induce statistics of natural imagery
- E.g. small pixel values, spatial smoothness

![img](imgs/M2L07_26.png)

*More info in Q&A: __How is Gradient Ascent done?__*

*More info in Q&A: __How is Gradient Ascent used in image optimization?__*

__Example Images__

![img](imgs/M2L07_27.png)
![img](imgs/M2L07_28.png)

Can improve results with __various tricks__:
- Clipping of small values & gradients
- Gaussian blurring

__Improved Results__

![img](imgs/M2L07_29.png)

### Summary
- We can optimize the input image to __generate__ examples to increase class scores or activations
- This can show us a great deal about what examples (not in the training set) __activate the network__


### References and Links:

- Simonyan et al., “Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps”, 2013
- Yosinski et al., “Understanding Neural Networks Through Deep Visualization”, 2015

## Testing Robustness

### Gradient Ascent on the Scores

- We can perform __gradient ascent__ on image
- Rather than start from zero image, why not real image?
- And why not optimize the score of an __arbitrary__ (incorrect!) class?

*__Surprising result__: You need very small amount of pixel changes to make the network confidently wrong!*


![img](imgs/M2L07_30.png)

### Example of Adversarial Noise

__Note this problem is not specific to deep learning!__
- Other methods also suffer from it
- Can show how __linearity__ (even at the end) can bring this about
	- Can add many small values that add up in right direction

![img](imgs/M2L07_31.png)

### Variations of Attacks

![img](imgs/M2L07_32.png)

*More info in Q&A: __What are white and black box attacks of CNN?__*

*More info in Q&A: __What are some common types of attacks on CNN? Include references.__*

### Summary of adversarial Attacks/Defenses

Similar to other security-related areas, it's an active __cat-and-mouse game__

__Several defenses such as:__
- Training with adversarial examples
- Perturbations, noise, or reencoding of inputs

There are __not universal methods__  that are robust to all types of attacks

### Other Forms of Robustness Testing

![img](imgs/M2L07_33.png)

### Analyzing Bias

We can try to undrstand the __biases of CNNs__
- Can compare to those of humans

__Example: Shape v.s. Texture Bias__

![img](imgs/M2L07_34.png)
![img](imgs/M2L07_35.png)

### Summary
- Various ways to test the __robustness__ and __biases__ of neural networks
- Adversarial examples have __implications__ for understanding and trusting them
- Exploring the __gains of different architectures__ in terms of robustness and biases can also be used to understand what has been learned

### References and Links:
- Simonyan et al., “Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps”, 2013
- Goodfellow et al., “Explaining and Harnessing Adversarial Examples”, 2015
- Su et al., “One Pixel Attack for Fooling Deep Neural Networks”, 2019.
- Chakraborty et al., Adversarial Attacks and Defences: A Survey, 2018
- Hendrycks & Dietterich, “Benchmarking Neural Network Robustness to Common Corruptions and Perturbations”, 2019.
- Geirhos, “ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness”, 2018.

## Style Transfer

### Generating Images with Content

- We can generate images throught backprop
	- Regularization can be used to ensure we match image statistics
- __Idea__: What if we want to preserve the content of the image?
	- Match features at different layers!
	- We can have a loss for this

![img](imgs/M2L07_36.png)

__Matching Features to Replicate Content__

![img](imgs/M2L07_37.png)

__Multiple Content Losses__

- How do we deal with multiple losses?
	- Remember, backwards edges going to same node *summed*
- We can have this content loss at many different layers and sum them too!

![img](imgs/M2L07_38.png)

### Replicating Content and Style

- __Idea__: Can we have the *content* of one image and *tesxture* (style) of another image?
	- __Yes!__

![img](imgs/M2L07_39.png)	

### Representing Texture
- How do we represent similarity in terms of textures?
- Long history in image processing!
	- Key ideas revolve around summary *statistics*
	- Should ideally remove most spatial information
- Deep learning variant: Feature correlations!
	- Called a __Gram Matrix__

__Gram Matrices__

![img](imgs/M2L07_40.png)	

*More info in Q&A: __In the context of CNN, what is a Gram Matrix used for? How is it calculated? Include references.__*

__Examples__

![img](imgs/M2L07_41.png)	
![img](imgs/M2L07_42.png)	

## Summary
- Generating images throught optimization is a powerful concept!
- Besides fun and art, methods such as stylization also useful for understanding what the network has learned
- Also useful for other things such as data augmentation




